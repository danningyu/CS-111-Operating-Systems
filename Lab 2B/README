NAME: Danning Yu
EMAIL: danningyu@ucla.edu
ID: 305087992

Included files:

SortedList.h: A header file supplied by the instructor containing the interface for a doubly linked list with a dummy node, containing operations to look up a node, insert a node, get the length of the list, and delete a node. Also contains the definition of a linked list node.

SortedList.c: A C source file containing my implementation of the 4 functions specified in the SortedList.h. The implementations also check for if the list was corrupted.

lab2_list.c: A C source file that uses the functions specified in SortedList.h and SortedList.c to insert and delete elements from a linked list in an analogous fashion to lab2_add, where every node added is eventually deleted. It also takes --iterations, --threads, and --sync arguments that behave the same as described in lab2_add except for the elimination of the atomic GCC function option, and the --yield argument now takes arguments to have it yield when inserting, looking up, and/or deleting a node. Finally, it takes a --lists argument, which partitions the linked list into multiple sublists as to parallelize the linked list operations.

lab2_list.gp: A gnuplot script for plotting the data generated by the Makefile test runs of the lab2_list program.

lab2b_list.csv: A comma-separated values file containing outputs of Makefile test runs for lab2-list with various combinations of thread, iteration, yield, sync, and list options.

profile.out: A execution profiling report of the lab2_list program run with --threads=12 --iterations=1000 --sync=s options to discover where the most time is spent in an unpartitioned spin-lock program execution.

lab2b_1.png: A graph of throughput versus number of threads for mutex and spin-lock synchronized linked list operations. Generated using data from lab2_list.

lab2b_2.png: A graph of average waiting time for a mutex and average time taken to carry out an operation versus number of threads for a program using mutex synchronization. Generated using data from lab2_list.

lab2b_3.png: A plot of successful iterations versus number of threads for the mutex and spin-lock synchronization methods. 4 sublists were used, along with yielding for inserting and deleting. Generated using data from lab2_list.

lab2b_4.png: Throughput versus number of threads for different numbers of sublists ranging from 1 to 16, and mutex synchronization. Generated using data from lab2_list.

lab2b_5.png: Throughput versus number of threads for different numbers of sublists ranging from 1 to 16, and spin-lock synchronization. Generated using data from lab2_list.

Makefile: A file containing the following targets:
	default: The default target, builds the lab2_list executable.

	tests: target that first builds the executables using the default target and then runs tests that vary the number of threads, number of operations, yielding or not yielding, syncing, and number of sublists. All results are exported to .csv files (lab2b_list.csv).

	graphs: target that uses gnuplot along with the lab2_list.gp script to create plots from the data lab2b_list.csv.

	profile: target that uses gperftools to profile the execution time and how much time is spent on each line for lab2_list run with spin-locking, 12 threads, and 1000 iterations (and 1 sublist).

	dist: target that runs the tests and then creates the graphs before making the submission tarball.

	clean: target that removes the lab2_list executable and tar files.

Answers to questions:

2.3.1:
For 1 and 2 thread lists, most of the cycles are spent in the list operations: inserting, getting the length, looking up an element, and deleting, as with few threads there is little or no contention for the linked list. For high-thread spin-lock tests, most of the time is spent spinning, and in high-thread mutex tests, most of the time is spent on context switches because there are many threads competing for the linked list, and so thus we will have to switch often between the threads.

2.3.2:
The lines of code that are consuming the most time are lab2_list.c:152 and lab2_list.c:227, which correspond to the spin-lock lines that acquire the lock before inserting and deleting from the linked list (the while test and set lines). Combined together they take up 97.4% of the total run time of hte program. This operation becomes expensive because with large numbers of threads, a lot of time is spent spinning, waiting for another thread to yield.

2.3.3:
The average lock-wait time rises dramatically because with more threads, all of them are contending for the same resource, thus increasing the wait time. On the other hand, the average time per operation rises but not as fast because even though other threads may be waiting, work is still being done on the linked list, so there is still progress being made. At the same time, other threads are waiting, but that time is hidden by the time that the work is being done on the linked list, whereas for the waiting time to acquire a lock, that time is exposed and thus causes a greater rate of increase in wait times as number of threads increases.

2.3.4:
As the number of lists increases, the throughput becomes more constant as the number of threads increases. This makes sense, as with each sublist becoming smaller, there is less contention for that resource. As the number of lists is increased, the throughput increases, but eventually, we will reach a point where the number of sublists is equal to the number of elements, and then the throughput can no longer increase. Looking at the graphs above, it seems like the throughput of an N-way partitioned list is NOT equal to the throughput of a single list with 1/N threads, and this is because with more partitions, each partitioned list becomes shorter, thus greatly decreasing the amount of contention that occurs and increasing the throughput. There will be more locks to deal with, which results in more overhead.

Sources used for this project:
Alexandre Tiard's discussion slides
http://man7.org/linux/man-pages/man3/pthread_create.3.html
http://man7.org/linux/man-pages/man3/pthread_join.3.html
https://computing.llnl.gov/tutorials/pthreads/
http://man7.org/linux/man-pages/man2/clock_gettime.2.html
https://gcc.gnu.org/onlinedocs/gcc-4.4.3/gcc/Atomic-Builtins.html
https://linux.die.net/man/1/gnuplot
http://man7.org/linux/man-pages/man3/malloc.3.html
https://www.geeksforgeeks.org/mutex-lock-for-linux-thread-synchronization/
https://docs.oracle.com/cd/E26502_01/html/E35303/ggecq.html
https://linux.die.net/man/3/getopt_long
http://man7.org/linux/man-pages/man3/atexit.3.html
https://github.com/gperftools/gperftools
